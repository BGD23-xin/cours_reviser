<h6>1.What’s the difference between symbolic AI and ML?</h6>

- Symbolic AI:  
  - Rule-based  
  - Logic processing  
  - Good for structured environments  
  - Struggles with complexity  
- Machine Learning:  
  - Data-driven  
  - Pattern recognition  
  - Adapts to new data  
  - Handles complex problems  

--

<h6>
2.What is overfitting?
</h6>

- Too specific to training data  

- Poor generalization  

- Solved by cross-validation, regularization  

--

3. ###### What are the main lessons of the Uber accident in Arizona?
- Safety and redundancy are crucial 

- Need for effective human monitoring

- Address regulatory and ethical challenges

- Recognize technology limitations
  
  

--

4. ###### What is the Human out of the loop problem?
- Minimal human involvement

- Loss of expertise 

- Accountability issues  

- Ethical and social considerations  

--

5. ###### ML is particularly good at…
- Pattern recognition (e.g., image and speech recognition)  

- Large-scale data analysis  

- Predictive modeling  

- Learning from examples and adapting  

--

6. ###### ML is bad at …
- Understanding context or common sense  

- Making judgments outside its trained data  

- Handling novel situations it wasn't trained on  

- Avoiding biases present in training data  

--

7. ###### Why ML is problematic for life and death or other serious decisions…
   
   •    Lack of understanding of ethical or moral nuances  
   •    High dependence on data quality and quantity   
   •    Difficulties in accountability and transparency   
   •    Potential biases in decision-making  

--

8. ###### What are "spurious correlations"?
   
   •    Misleading statistical correlations between two variables   
   •    No causal relationship despite a strong correlation   
   •    Often arise in large datasets with many variables   
   •    Highlight the need for careful data analysis and interpretation  

--

9. ###### What’s the trolley problem?
   
   •    Ethical dilemma in philosophy   
   •    Choice between two harmful actions   
   •    Highlights moral decision-making complexities  

--

10. What is the “wolf and snow” problem?

--

11. Can ML be certified in safety critical systems?  
    •    Certification is challenging  
    •    Requires rigorous testing  
    •    Needs reliability, robustness, transparency  

--

12. What are the main kinds (and sources) of algorithmic bias?  
    •    Types: Pre-existing, technical, emergent  
    •    Sources: Biased data, model assumptions, societal stereotypes  

--

13. What are the different kinds of fairness in the "orange and blue dots" exercise?  
    •    Procedural Fairness: Fair process  
    •    Distributive Fairness: Fair outcomes  
    •    Interactional Fairness: Fair interpersonal treatment  

--

14. Are individual fairness and group fairness compatible?  
    •    Individual Fairness: Equal treatment for individuals  
    •    Group Fairness: Equal treatment for groups  
    •    Compatibility: Often challenging, trade-offs may be necessary  

--

15. What are the two main facial recognition use cases, and why are their risk profiles different?  
    •    Main Use Cases: Security and identification, personalization and marketing  
    •    Different Risk Profiles: Security uses have privacy, ethics concerns; marketing uses raise consent, bias issues  

--

16. What are the consequences of false positives, or false negatives, in different scenarios, and how should you balance between the two?  
    •    False Positives: Unjustly flagging an innocent in security, unnecessary medical treatment.  
    •    False Negatives: Overlooking a threat in security, missing a medical diagnosis.  
    •    Balancing: Depends on context; prioritize reducing the more harmful error.  

--

17. ###### What are some ways to reduce algorithmic bias?
- Diverse and representative data  

- Regular audits for bias  

- Transparent and explainable models  

- Continuous feedback and adjustments  

--

18. ###### Is every case of algorithmic bias also an unlawful discrimination? What's the difference between bias and illegal discrimination?
    
    •    Not all algorithmic bias is illegal discrimination.   
    •    Illegal discrimination involves unfair treatment based on protected characteristics like race, gender.   
    •    Bias can be subtle and not always protected by law.  

--

19. To eliminate discrimination, is it sufficient to remove "protected attributes" from the data?  
    •    Removing protected attributes is not always sufficient.  
    •    Bias can persist indirectly through correlated features.  
    •    Requires a more holistic approach.  

--

20. Can you reduce false positives and false negatives at the same time?  
    •    Often a trade-off; improving one can worsen the other.  
    •    Balance depends on the application and its consequences.  

--

21. What is ClearviewAI, and why is it problematic? Is it legal?  
    •    Facial recognition technology company.  
    •    Problematic due to privacy concerns, ethical issues.  
    •    Legal status varies by region and is subject to ongoing debate.  

--



22. What's an example of a "feedback loop" problem in algorithmic bias?  
    •    Example: Predictive policing leading to more patrols in certain areas, increasing arrests, reinforcing the bias.  

--

23. Are all algorithmic errors also a bias? What's the difference?  
    •    Not all errors are biases.  
    •    Errors can be random or due to technical issues.  
    •    Bias implies a systematic skew in one direction.  

--

Questions from Sept 16 class (VALÉRIE BEAUDOUIN)  

24. What is COMPAS and what were the issues surrounding its deployment?  
    •    Correctional Offender Management Profiling for Alternative Sanctions.  
    •    A software used for assessing the likelihood of a criminal reoffending.  
    •    Issues: Accused of racial bias and lack of transparency in its algorithms.  
    
    

--

25. What is transhumanism?  
    •    Philosophical movement advocating for enhancing human abilities through technology.  
    •    Envisions a future where humans transcend natural limitations.  
    •    Topics include life extension, cognitive enhancement, and AI integration.  

--

26. What is singularity?  
    •    Hypothetical point in time when AI surpasses human intelligence.  
    •    Associated with dramatic technological and societal changes.  
    •    Often debated for its feasibility and potential impacts.  

--

27. What is symbolic vs connectionist AI?  
    •    Symbolic AI: Rule-based, logical reasoning. Examples: Expert systems.  
    •    Connectionist AI (Neural Networks): Learning from data, pattern recognition. Examples: Deep learning models.  

--

28. What regions of the world are most involved in debates on AI ethics?  
    •    North America and Europe: Often leading discussions on privacy, bias, and regulation.  
    •    Asia, particularly China and Japan: Focused on AI's role in economic growth, social stability, and cultural perspectives.  
    •    Global South: Increasing participation, raising issues of equitable AI development and impacts on developing economies.  

---

Questions from Sept 19 2022 class (WINSTON MAXWELL)  



29. ###### Moral approaches: "utilitarian vs rights/virtue"
    
    •    Utilitarianism: Focuses on outcomes, maximizing overall happiness or utility.   
    •    Rights/Virtue Ethics: Emphasizes moral rights of individuals, moral character.  

--

30. ###### Fundamental rights, are they all absolute?
    
    Generally, ***<u>no</u>***. Most fundamental rights are not absolute and can be limited under specific circumstances, especially when they conflict with other rights or public interests.  

--



31. Name several fundamental rights that are often involved in AI cases.  
    •    Right to privacy and data protection  
    •    Freedom of expression  
    •    Non-discrimination and equality  
    •    Right to work and fair conditions of employment  

--

32. ###### Fundamental rights generally protect the individual vis à vis the government (True/false)
    
    ***<u>True</u>***. Fundamental rights primarily protect individuals from government actions, but they also have implications for private entities, especially in contexts like employment, service provision, and use of personal data.  

--

33. ###### Can fundamental rights be balanced?
    
    ***<u>Yes</u>***, fundamental rights can be balanced against each other and against collective interests. 
    This is often necessary when rights come into conflict or in situations involving public safety, national security, or other public interests  

--

34. ###### What is the proportionality test?
- A legal principle used to determine if the limitation of a fundamental right is justified.   

- Involves assessing:
  
  - Legitimacy of the objective.  
  
  - Suitability of the measure for achieving this objective.  
  
  - Necessity (i.e., if there are less restrictive means available).  
  
  - Balance between the infringement of rights and the importance of the objective.  

--

35. What is human dignity?  
    •    A fundamental moral and legal principle.  
    •    Involves respect for the intrinsic worth of every individual.  
    •    Often underpins human rights legislation and ethical considerations.  

--

36. What was the “dwarf-throwing” case about?  
    •    A controversial legal case from France.  
    •    Involved a ban on "dwarf-throwing" activities for reasons of human dignity.  
    •    Raised questions about personal autonomy vs. societal values of dignity.  

--

###### 37.A government surveillance system to detect terrorism : please apply the proportionality test- what are the rights at stake, what are the steps in the proportionality test?

- Rights at Stake: Privacy, freedom of expression, non-discrimination.
- Steps:  
  - Objective: Preventing terrorism.  
  - Suitability: Surveillance can be effective for this purpose.  
  - Necessity: Assess if less intrusive methods could achieve the same goal.  
  - Balancing: Weighing the rights infringed against the importance of security.  

--

38. Base rate fallacy (Wikipedia): why algorithms generate a high number of false positives  
    •    The fallacy occurs when the rarity of an event (like terrorism) is not considered.  
    •    In a large population, even a highly accurate test will yield many false positives when detecting a rare event.  
    •    This is a challenge for algorithms in scenarios like security screening, where the base rate of actual threats is very low.  

--



39. Are ethics charters within large companies useful?  
    •    Useful for setting standards and guiding behavior.  
    •    Reflect company values and commitment to ethical practices.  
    •    Influence corporate culture and decision-making.  

--

40. Do they create legally binding rules?  
    •    Typically not legally binding.  
    •    Serve more as guidelines or statements of intent.  
    •    However, they can have legal implications if integrated into contracts or company policies.  

--

41. What needs to accompany ethics charters for them to be effective?  
    •    Requires enforcement mechanisms.  
    •    Needs clear guidelines and training.  
    •    Should be integrated into company culture and decision-making processes.  
    •    Regular review and updates to stay relevant.  

--

42. What are the seven principles in the HLEG guideines?  
    •    Human agency and oversight.  
    •    Technical robustness and safety.  
    •    Privacy and data governance.  
    •    Transparency.  
    •    Diversity, non-discrimination, and fairness.  
    •    Societal and environmental well-being.  
    •    Accountability.  

--

43. Does the proposed AI Act prohibit all “high risk” AI systems?  
    •    The proposed AI Act does not prohibit all high-risk AI systems.  
    •    It aims to regulate and ensure safe and ethical use of such systems.    
    •    Includes requirements for transparency, accountability, and data governance.  

--

44. What are some elements in the “risk management system” that would be imposed by the future AI Act?  
    •    Continuous risk assessment and mitigation strategies.  
    •    Documentation and reporting procedures.  
    •    Quality and safety checks throughout AI system lifecycle.  
    •    Mechanisms for human oversight and intervention.  

--

45. What are the three main objectives of laws that regulate AI and big data?  
    •    Protect individual rights and privacy.  
    •    Ensure transparency and accountability in AI systems.  
    •    Promote fair and non-discriminatory use of AI and data.  

--

46. Give examples of threats to the human democratic ecosystem.  
    •    Misinformation and propaganda spread through social media.  
    •    Surveillance and privacy intrusions by governments or corporations.  
    •    Algorithmic biases leading to discrimination or inequality.  

--

47. What’s the “ratchet effect” (effet cliquet)?  
    •    A situation where something can easily move in one direction but is difficult or impossible to move back.  
    •    Often used in economics and policy to describe how certain measures, once introduced, become permanent or irreversible.  

--

48. What is transhumanism?  
    •    Philosophical movement that advocates enhancing human capacities through advanced technology.  
    •    Envisions a future where humans transcend their biological limitations.  
    •    Involves concepts like life extension, cognitive enhancements, and AI integration  

--

49. What is the difference between global and local explanations?  
    •    Global Explanations: Provide an overview of how an AI model works in general.  
    •    Local Explanations: Explain a specific decision or prediction made by an AI model.  

--

###### 50.True/False: Post-hoc explanations give a 100% faithful indication of the reasons for an algorithmic decision.

***<u>False</u>***. Post-hoc explanations do not guarantee a 100% faithful indication of the reasons for an algorithmic decision. They often provide an approximation or interpretation of the decision-making process.  

--

51. Give an example of a hybrid explanation method.  
    •    Combines global (e.g., decision trees for overall model understanding) and local explanations (e.g., LIME for specific predictions).  
    •    Provides both broad and detailed insights into AI decision-making.  

--

52. What is automation bias.  
    •    Tendency to over-rely on automated systems.  
    •    Can lead to errors, especially when automated suggestions are incorrect.  
    •    Significant in critical fields like aviation and healthcare.  

---



Questions relating to Jean-Louis Dessalles course  

53. What is the main operation that allows NN-based ML to generalize from examples ?  
    •    Uses training on diverse datasets to learn patterns and make predictions on unseen data.  
    •    Generalization facilitated by layers of interconnected nodes (neurons) that process features at different levels of abstraction.  

--

54. Image recognition through NN is good at recognizing objects, but have a hard time recognizing...  
    •    Struggles with context, relationships between objects, and subtle nuances that require deep understanding.  
    •    Often challenged by images with unusual perspectives or occlusions.  

--

55. What are the three main AI techniques used in alpha-Go?  
    •    Deep Neural Networks for pattern recognition and learning board positions.  
    •    Monte Carlo Tree Search for simulating game scenarios and strategy planning.  
    •    Reinforcement Learning for improving through self-play and feedback.  

--

56. Why are adversarial examples so efficient in fooling NNs?  
    •    Exploit the way NNs process information; small changes unnoticeable to humans can significantly alter NN output.  
    •    NNs often lack the broader context or understanding, making them vulnerable to carefully crafted perturbations.  

--

57. Should we correct bias by changing the data, by modifying the loss function, by adding a "safety net" downstream?  
    •    Approach depends on context and type of bias.  
    •    Changing data: Ensuring diversity and representativeness.  
    •    Modifying loss function: Penalizing biased predictions.  
    •    Safety net: Implementing checks and balances post-processing.  

--

58. How can a solution to a typical IQ test be ideally recognized as "relevant"?  
    •    Ideally through understanding underlying patterns, logic, or relationships in the test.  
    •    Requires advanced cognitive and pattern recognition abilities, potentially leveraging techniques like natural language processing and logical inference.  
    
    

--

59. What is the main interesting property of latent spaces?  
    •    Encode high-dimensional data into a lower-dimensional space, capturing intrinsic patterns and relationships.  
    •    Facilitates tasks like feature extraction, data compression, and uncovering hidden factors.  

--

60. How does NN-based ML adapt to context?  
    •    Learning from context-specific data.  
    •    Utilizing recurrent neural networks (RNNs) or attention mechanisms to process sequential or time-dependent data.  

--

61. What is the main reason why social "bubbles" emerge?  

•    Algorithmic filtering and personalization leading to exposure to similar viewpoints and information, reinforcing existing beliefs.  

--

62. Can we think of an easy way to better preserve privacy?  

•    Using techniques like differential privacy, where noise is added to the data or queries to protect individual identities while still providing useful aggregate information.  
•    Ensuring data minimization and anonymization  

---



Questions relating to Jean-Samuel Beuscart course  

63. What is a filter bubble?  
    •    An environment where a user is exposed to limited information based on personalized algorithms, often reinforcing existing views.  

--

64. What is a long tail?  
    •    Refers to the strategy of selling a large number of unique items with relatively small quantities sold of each, typically in contrast to selling a small number of popular items in large volumes.  
    
    

--



65. What is a hype loop?  
    •    This term seems unusual in the context of AI or technology. It might be a misinterpretation or a less common term. If you meant "hype cycle," it refers to a graphical representation of the maturity, adoption, and social application of specific technologies.  

--

66. What is stickiness?  
    •    A measure of a product's or service's ability to retain customers over time. In online contexts, it often refers to how engaging a website or app is, encouraging repeat visits and prolonged use.  

--

67. Do music recommendation systems increase diversity/long tail music listening?  
    •    Can both increase and decrease diversity; they may expose listeners to a wider range of music (long tail effect) but can also reinforce popular choices, leading to a concentration on a limited number of tracks or artists.  

--

68. What is the approximate distribution of music consumption (% artists that occupy most music streaming)?  
    •    Typically follows a Pareto distribution, where a small percentage of artists (often around 20%) account for a large percentage (up to 80%) of total music streaming.  

--

69. What are the key metrics to optimise for a commercial recommender system?   
    •    Click-through rates, conversion rates, user engagement, retention, and revenue generation.  

--

70. What might be the metrics for a public service recommender system?  
    •    User satisfaction, content diversity, fairness, inclusivity, and educational or cultural value.  
    
    

---

Questions relating to Ada Diaconescu's class  

--

###### 71.You should be prepared to talk about a "technology" and its various effects.

 When discussing a technology, consider multiple aspects:  

- **<u>Functionality</u>**: How the technology works, its capabilities, and limitations.  

-  **<u>Impact on Society</u>**: Effects on social interactions, lifestyle, work, education, and culture.  

- **<u>Economic Effects</u>**: Influence on markets, employment, productivity, and innovation.  

--

* **<u>Ethical and Legal Considerations</u>**: Issues of privacy, data security, ethical use, and regulatory implications.

* <u>**Environmental Impact**</u>: Its effect on resources, energy consumption, and ecological footprint.

* **<u>Future Prospects</u>**: Potential developments, emerging trends, and long-term implications.

--

* For example, discussing "Artificial Intelligence":

* **<u>Functionality</u>**: AI involves machine learning, natural language processing, robotics, etc.

* <u>**Social Impact**</u>: Automates tasks, influences decision-making, changes human interaction.

* **<u>Economic Effects</u>**: Creates new industries, disrupts job markets, improves efficiencies.

--

* **<u>Ethical/Legal</u>**: Raises concerns about privacy, bias, accountability, and the need for regulation.

* **<u>Environmental Impact</u>**: Energy-intensive, especially large models, but also enables efficient resource management.

* **<u>Future Prospects</u>**: Continued integration into daily life, ethical challenges, potential for both positive and negative impacts on society.

---



Questions relating to Christian Licoppe's class  

72. What's the Turing test, and the Imitation Game?  
    •    Turing Test: A measure of a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.  
    •    Imitation Game: Proposed by Alan Turing, a test where an interrogator tries to distinguish between a human and a machine based on their replies to questions.  

--

73. What are Eliza and Lenny?  
    •    Eliza: An early natural language processing computer program created at MIT, simulating a psychotherapist.  
    •    Lenny: A chatbot designed to mimic human conversation, often used to waste telemarketers' time with its convincing, looped script.  

--

74. What are the objectives/functions of a "conversation"?  
    •    Exchange of information.  
    •    Social bonding and relationship building.  
    •    Persuasion or influence.  
    •    Entertainment and enjoyment.  

--

75. What's the "vallée de l'étrange" (uncanny valley) ?  
    •    A hypothesis in robotics and 3D animation stating that as a robot or character becomes more human-like, it evokes increasingly positive responses until it reaches a point where the likeness is almost human, causing a sharp negative reaction.  

--

76. How do we close a human interaction?  
    •    Typically involves signaling that the conversation is ending (e.g., summarizing, farewells).  
    •    Often includes non-verbal cues like body language.  

--

77. What differences do we see in closure of interactions between humans and robots, and why?  
    •    Robots may lack the subtlety and nuance in closing interactions, such as picking up on social cues.  
    •    Human-robot interactions might end more abruptly or mechanically, lacking the warmth or personal touch found in human-to-human interactions.  
    •    This difference stems from robots' current limitations in understanding and replicating complex human social behaviors.  
    
    
    
    

---



Questions relating to Antonio Casilli's class  

---

78. ###### What are the key principles contained in "AI Guidelines" and what is missing?
    
    •    Transparency: Making AI systems and their decisions understandable.   
    
    •    Accountability: Ensuring responsibility for AI outcomes.   
    
    •    Fairness: Avoiding biases and promoting equality.   

--

- Safety and Security: Protecting against misuse and ensuring reliability.

-  Privacy: Safeguarding personal data.

-  What's Missing: Often lacks specific implementation strategies, enforcement mechanisms, and may not fully address the rapid evolution of AI technology.

--

79. What does the term "capture" refer to?    
    •    Refers to the process of obtaining data or information, like image capture in photography or data capture in research. In a broader   context, it can also imply the undue influence of one group over a system or set of regulations.    

--

80. Why was Timnit Gebru dismissed from Google?  
    •    Timnit Gebru, a leading AI ethics researcher, was dismissed following a controversy over a research paper she co-authored. The paper discussed the risks of large language models, like those used by Google, including ethical considerations, environmental impact, and potential biases.    

--

81. What is the "Moral Machine" experiment?  
    •    An online platform that explores moral dilemmas faced by autonomous vehicles. Participants are asked to decide outcomes in various scenarios, revealing public preferences on ethical decisions in AI.  

--

82. What are the main characteristics of the Uber autonomous vehicle and how it learned?  
    •    Uses a combination of sensors like LIDAR, radar, and cameras for navigation and obstacle detection.  
    •    Learns through machine learning algorithms, often trained on large datasets from real-world driving scenarios and simulations.  
    
    

--



83. What are the main issues relating to Amazon Mechanical Turk?  
    •    Concerns include low pay, lack of worker protections, and potential for exploitation.  
    •    Ethical concerns over task transparency and the use of human labor in tasks presented as AI-driven.  

--



84. What purpose does Captcha serve in addition to ensuring you're a human?  
    •    Apart from verifying users as human, Captcha also helps in digitizing text, annotating images, and improving machine learning models. It utilizes human responses to decode ambiguous or distorted information that computers struggle to understand.  

---

Questions relating to Caroline Rizza's class.  

--

85. ###### How do choices made in terms of development and uses of technology have long term impacts on society?
- Societal Change: Technologies can fundamentally alter how societies function, communicate, and interact. 

- Economic Impact: Shifts in job markets, creation of new industries, economic inequalities. 

- Ethical and Moral Implications: New technologies often challenge existing ethical norms and legal frameworks. 

--

* Environmental Impact: Technologies can have lasting effects on the environment, both positive and negative.

* Privacy and Security: Long-term implications for individual privacy and data security.

--

86. What was the Google projet Maven controversy about?  
    •    Project Maven was a Pentagon project that Google participated in, involving the use of AI for analyzing drone footage.  
    •    Controversy arose due to ethical concerns over the use of AI in military applications, leading to protests from Google employees and the tech community.  

--



87. What was the Microsoft IVAS augmented vision controversy about?  
    •    The Integrated Visual Augmentation System (IVAS) was a project between Microsoft and the U.S. Army to develop augmented reality headsets for soldiers.  
    •    The controversy centered around ethical concerns regarding the use of augmented reality in warfare and the potential consequences of such technologies.  

--

88. What's Onco-Mouse?  
    •    OncoMouse refers to a genetically modified mouse that was developed to carry a specific gene making it susceptible to cancer, used for cancer research.  
    •    It raised ethical questions about genetic modification and animal rights in scientific research.  

--

89. What was the Facebook papers controversy about?  
    •    The Facebook Papers were a series of internal documents leaked by a whistleblower, revealing how Facebook (now Meta) handled   misinformation, hate speech, and mental health issues related to its platforms.  
    •    The controversy highlighted the company's struggle with content moderation, its impact on public discourse and mental health, and the balance between free speech and harmful content.  
